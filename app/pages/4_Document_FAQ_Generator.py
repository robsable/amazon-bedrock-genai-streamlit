import streamlit as st
from datetime import datetime, timezone, timedelta

from langchain.prompts import PromptTemplate
from langchain_community.llms import Bedrock
from langchain.chains.summarize import load_summarize_chain
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

from pages.lib import models_shared

# def get_llm():
    
#     model_kwargs = { #AI21
#         "maxTokens": 8000, 
#         "temperature": 0, 
#         "topP": 0.5, 
#         "stopSequences": [], 
#         "countPenalty": {"scale": 0 }, 
#         "presencePenalty": {"scale": 0 }, 
#         "frequencyPenalty": {"scale": 0 } 
#     }
    
#     llm = Bedrock(
#         region_name="us-west-2",
#         model_id="ai21.j2-ultra-v1", #set the foundation model
#         model_kwargs=model_kwargs) #configure the properties for Claude
    
#     return llm

def get_docs(doc_selection):
    
    loader = PyPDFLoader(file_path=doc_selection)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ".", " "], chunk_size=4000, chunk_overlap=0 
    )
    docs = text_splitter.split_documents(documents=documents)
    
    return docs


def get_summary(model_id=None, temperature=0.0, return_intermediate_steps=False, doc_selection=None):
    
    map_prompt_template = "{text}\n\nWrite an expected list of frequently asked questions and their answers based on the above content. List each question on a new line beginning with Q:. Then leave a blank line. On a new line below that, list each answer beginning with A: "
    map_prompt = PromptTemplate(template=map_prompt_template, input_variables=["text"])
    
    llm, is_chat = models_shared.get_llm(model_id, temperature)
    docs = get_docs(doc_selection=doc_selection)
    
    chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=map_prompt, return_intermediate_steps=return_intermediate_steps)
    
    if return_intermediate_steps:
        return chain.invoke({"input_documents": docs}, return_only_outputs=True)
    else:
        return chain.invoke(docs, return_only_outputs=True)

#################
# Streamlit App #
#################
st.set_page_config(layout="wide", page_title="Product Summary & FAQs", page_icon=":interrobang:")
st.title("Product Summary & FAQs")
st.caption("**Instructions:**  (1) Select a document  (2) Click Generate")

# Get Bedrock LLM Models options
model_options = list(models_shared.model_options_dict)

timezone_offset = -4.0  # Eastern Standard Time (UTCâˆ’08:00)
tzinfo = timezone(timedelta(hours=timezone_offset))

col1, col2 = st.columns([.25,.75])

pdf_options_dict = {
    "./pages/data/beginners-guide.pdf" : "AWS Beginners Guide",
    "./pages/data/awsgsg-intro.pdf" : "AWS Intro",
}
pdf_options = list(pdf_options_dict)

doc_selection = st.radio("**Select a Document:**", pdf_options, format_func=pdf_options_dict.get, horizontal=True)
return_intermediate_steps = st.checkbox("Generate FAQs", value=True)
selected_model = st.radio("**Select a model:**", 
    model_options,
    format_func=models_shared.get_model_label,
    horizontal=True
)
summarize_button = st.button("Generate", type="primary")

if summarize_button:
    st.subheader("Document summary")
    st.write("**GENERATED BY:** "+selected_model)

    start = datetime.now(tzinfo)

    with st.spinner("Running..."):
        response_content = get_summary(model_id=selected_model, temperature=0.0, return_intermediate_steps=return_intermediate_steps, doc_selection=doc_selection)

    end = datetime.now(tzinfo)
    st.write("**TIME ELAPSED** = " + str(end - start))

    if return_intermediate_steps:

        st.write(response_content["output_text"])

        st.subheader("Section FAQs")

        for step in response_content["intermediate_steps"]:
            st.write(step)
            st.markdown("---")

    else:
        st.write(response_content["output_text"])

